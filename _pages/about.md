---
permalink: /
title: "About me"
# excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Howdy! I completed my PhD in Computer Science at UT Austin, advised by Prof. Sujay Sanghavi and Prof. Inderjit S. Dhillon. I will join Google Research as a Research Scientist. I am interested in developing provably better optimization algorithms and generalization-improving techniques for machine learning (ML), especially under data-centric constraints. In general, I like to develop theoretically grounded ML algorithms.

Previously, I received the combined B.Tech. and M.Tech. degree in Electrical Engineering from IIT Bombay. Here I worked under the guidance of Prof. Subhasis Chaudhuri and I was awarded the Undergraduate Research Award.

You can check out my outdated CV [here]({{ site.url }}/assets/CV_website.pdf). My email is rdas(at)utexas(dot)edu.

**<font size="+3">Papers</font>**

* "**Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting**" - Sunny Sanyal^, Hayden Prairie^, **Rudrajit Das**^, Ali Kavis^, and Sujay Sanghavi (^ denotes equal contribution).

* "**Retraining with Predicted Hard Labels Provably Increases Model Accuracy**" - **Rudrajit Das**, Inderjit S. Dhillon, Alessandro Epasto, Adel Javanmard, Jieming Mao, Vahab Mirrokni, Sujay Sanghavi, and Peilin Zhong.

    Preprint. Download <a href="https://arxiv.org/abs/2406.11206" style="color: #0000FF">here</a>.

* "**Towards Quantifying the Preconditioning Effect of Adam**" - **Rudrajit Das**, Naman Agarwal, Sujay Sanghavi, and Inderjit S. Dhillon.

    Preprint. Download <a href="https://arxiv.org/abs/2402.07114" style="color: #0000FF">here</a>.

* "**Understanding the Training Speedup from Sampling with Approximate Losses**" - **Rudrajit Das**, Xi Chen, Bertram Ieong, Parikshit Bansal, and Sujay Sanghavi.

    ICML 2024. Download paper <a href="https://openreview.net/pdf/d3f58fe5de95e47b4f05e4c964d14b536ec6d510.pdf" style="color: #0000FF">here</a>.

* "**Understanding Self-Distillation in the Presence of Label Noise**" - **Rudrajit Das** and Sujay Sanghavi.
   
   ICML 2023. Download paper <a href="https://proceedings.mlr.press/v202/das23d/das23d.pdf" style="color: #0000FF">here</a>.

* "**On the Unreasonable Effectiveness of Federated Averaging with Heterogeneous Data**" - Jianyu Wang, **Rudrajit Das**, Gauri Joshi, Satyen Kale, Zheng Xu, and Tong Zhang.

    TMLR. Download paper <a href="https://openreview.net/pdf?id=zF76Ga4EPs" style="color: #0000FF">here</a>.

* "**Beyond Uniform Lipschitz Condition in Differentially Private Optimization**" - **Rudrajit Das**, Satyen Kale, Zheng Xu, Tong Zhang, and Sujay Sanghavi.

    ICML 2023. Download paper <a href="https://proceedings.mlr.press/v202/das23c/das23c.pdf" style="color: #0000FF">here</a>. 
    
* "**Differentially Private Federated Learning with Normalized Updates**" - **Rudrajit Das**, Abolfazl Hashemi, Sujay Sanghavi, and Inderjit S. Dhillon.

    Download preprint <a href="https://arxiv.org/pdf/2106.07094.pdf" style="color: #0000FF">here</a>. Short version presented in OPT2022 workshop of NeurIPS 2022; download <a href="https://openreview.net/pdf?id=0FllaTqjor7" style="color: #0000FF">here</a>.

* "**Faster Non-Convex Federated Learning via Global and Local Momentum**" - **Rudrajit Das**, Anish Acharya, Abolfazl Hashemi, Sujay Sanghavi, Inderjit S. Dhillon, and Ufuk Topcu.

    UAI 2022. Download paper <a href="https://proceedings.mlr.press/v180/das22b.html" style="color: #0000FF">here</a> and preprint <a href="https://arxiv.org/pdf/2012.04061.pdf" style="color: #0000FF">here</a>.

* "**On the Benefits of Multiple Gossip Steps in Communication-Constrained Decentralized Optimization**" - Abolfazl Hashemi, Anish Acharya^, **Rudrajit Das**^, Haris Vikalo, Sujay Sanghavi, and Inderjit Dhillon (^ denotes equal contribution).

    IEEE Transactions on Parallel and Distributed Systems. Download paper <a href="https://ieeexplore.ieee.org/abstract/document/9664349" style="color: #0000FF">here</a> and preprint <a href="https://arxiv.org/pdf/2011.10643.pdf" style="color: #0000FF">here</a>.
    
* "**On the Convergence of a Biased Version of Stochastic Gradient Descent**" - **Rudrajit Das**, Jiong Zhang, and Inderjit Dhillon.

     NeurIPS 2019 Beyond First Order Methods in ML workshop. Download paper <a href="https://drive.google.com/file/d/1fn1iO_CXWJzgVqiY3p84NfCf__i6hKCW/view" style="color: #0000FF">here</a>.

* "**On the Separability of Classes with the Cross-Entropy Loss Function**" - **Rudrajit Das** and Subhasis Chaudhuri.

    Preprint. Download <a href="https://arxiv.org/abs/1909.06930" style="color: #0000FF">here</a>.
    
* "**Nonlinear Blind Compressed Sensing under Signal-Dependent Noise**" - **Rudrajit Das** and Ajit Rajwade.

    IEEE International Conference on Image Processing (ICIP) 2019. Download paper <a href="https://ieeexplore.ieee.org/abstract/document/8803173" style="color: #0000FF">here</a>.

* "**Sparse Kernel PCA for Outlier Detection**" - **Rudrajit Das**, Aditya Golatkar, and Suyash Awate.

    IEEE International Conference on Machine Learning and Applications (ICMLA) 2018 Oral. Download paper <a href="https://arxiv.org/abs/1809.02497" style="color: #0000FF">here</a>.

* **iFood Challenge, FGVC Workshop, CVPR 2018** - Parth Kothari^, Arka Sadhu^, Aditya Golatkar^, and **Rudrajit Das**^ (^ denotes equal contribution).

    Finished $2^{nd}$ in the public leaderboard and $3^{rd}$ in the private leaderboard (Team name: Invincibles). <a href="https://www.kaggle.com/c/ifood2018/leaderboard" style="color: #0000FF">Leaderboard Link</a>.
    Invited to present our method at CVPR 2018 (slides can be found  <a href="https://drive.google.com/file/d/1ycgDwlw62mWgaLy5qslvqjyiND0vgYTG/view?usp=sharing" style="color: #0000FF">here</a>).

**<font size="+3">Internships</font>**

* **Student Researcher at Google Research, New York City, NY** (June '24 - August '24) <br/>
    *Host: Kyriakos Axiotis* <br/>
    * Worked on improving the quality of pruned large language models.  <br/>

* **Student Researcher at Google Research (Remote)** (November '23 - March '24) <br/>
    *Host: Alessandro Epasto* <br/>
    * Theoretically showed that retraining with predicted hard labels improves model accuracy in the presence of label noise. Empirically we showed that retraining appropriately can significantly improve training with label differential privacy.  <br/>
    
* **Student Researcher at Google DeepMind, Princeton, NJ** (June '23 - October '23) <br/>
    *Host: Naman Agarwal* <br/>
    * Derived new theoretical results to quantify the preconditioning effect of the Adam optimizer, and empirically benchmarked several optimization algorithms based on Adam. <br/>
    
* **Research Intern at Google (Remote)** (June '21 - August '21) <br/>
    *Hosts: Zheng Xu, Satyen Kale, and Tong Zhang* <br/>
    * Clipped gradient methods are commonly used in practice for differentially private (DP) training, e.g., DP-SGD. However, a sound theoretical understanding of these methods has been elusive. We provide principled guidance on choosing the clipping threshold in DP-SGD and also derive novel convergence results for DP-SGD in heavy-tailed settings. <br/>

* **Applied Scientist Intern at Amazon Search, Berkeley, CA** (May '20 - August '20) <br/>
    *Mentor: Dan Hill, Manager: Sujay Sanghavi* <br/>
    * Worked on customer-specific query correction by leveraging the "session data" (i.e. previous searches of the customer) using SOTA Transformer models. Our model generated better candidates than the production system. <br/>

* **Institute for Biomechanics, ETH Zürich, Zürich, Switzerland** (May '17 - July '17) <br/>
    *Guide : Dr. Patrik Christen and Prof. Dr. Ralph Müller, D-HEST* <br/>
    * Proposed a stable linear model (with closed-form solution) and a fuzzy boolean network for bone remodeling. Also developed an automated 2D-3D image registration framework for histology images from scratch. <br/>
