---
permalink: /
title: "About me"
# excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Howdy! I am a **research scientist** at **Google Research**. I am interested in developing provably better optimization algorithms and generalization-improving techniques for machine learning (ML), especially under data-centric constraints. In general, I like to develop theoretically grounded ML algorithms.

I recently completed my PhD in Computer Science at UT Austin, advised by Prof. Sujay Sanghavi and Prof. Inderjit S. Dhillon. Before that, I received the combined B.Tech. and M.Tech. degree in Electrical Engineering from IIT Bombay. Here I worked with Prof. Subhasis Chaudhuri and received the Undergraduate Research Award.

You can check out my outdated CV [here]({{ site.url }}/assets/CV_May_2025.pdf). My email is rdas(at)utexas(dot)edu.

**<font size="+3">Papers</font>**

* "**Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing**" - A Javanmard, **R Das**, A Epasto, and V Mirrokni.

    Preprint. Download <a href="https://arxiv.org/abs/2505.15195" style="color: #0000FF">here</a>.

* "**Upweighting Easy Samples in Fine-Tuning Mitigates Forgetting**" - S Sanyal\*, H Prairie\*, **R Das**\*, A Kavis\*, and S Sanghavi (\* denotes equal contribution).

    ICML 2025 (*spotlight*). Download preprint <a href="https://arxiv.org/abs/2502.02797" style="color: #0000FF">here</a>.

* "**Retraining with Predicted Hard Labels Provably Increases Model Accuracy**" - **R Das**, I S Dhillon, A Epasto, A Javanmard, J Mao, V Mirrokni, S Sanghavi, and P Zhong.

    ICML 2025. Download preprint <a href="https://arxiv.org/abs/2406.11206" style="color: #0000FF">here</a>.

* "**Towards Quantifying the Preconditioning Effect of Adam**" - **R Das**, N Agarwal, S Sanghavi, and I S Dhillon.

    Preprint. Download <a href="https://arxiv.org/abs/2402.07114" style="color: #0000FF">here</a>.

* "**Understanding the Training Speedup from Sampling with Approximate Losses**" - **R Das**, X Chen, B Ieong, P Bansal, and S Sanghavi.

    ICML 2024. Download paper <a href="https://openreview.net/pdf/d3f58fe5de95e47b4f05e4c964d14b536ec6d510.pdf" style="color: #0000FF">here</a>.

* "**Understanding Self-Distillation in the Presence of Label Noise**" - **R Das** and S Sanghavi.
   
   ICML 2023. Download paper <a href="https://proceedings.mlr.press/v202/das23d/das23d.pdf" style="color: #0000FF">here</a>.

* "**On the Unreasonable Effectiveness of Federated Averaging with Heterogeneous Data**" - J Wang, **R Das**, G Joshi, S Kale, Z Xu, and T Zhang.

    TMLR. Download paper <a href="https://openreview.net/pdf?id=zF76Ga4EPs" style="color: #0000FF">here</a>.

* "**Beyond Uniform Lipschitz Condition in Differentially Private Optimization**" - **R Das**, S Kale, Z Xu, T Zhang, and S Sanghavi.

    ICML 2023. Download paper <a href="https://proceedings.mlr.press/v202/das23c/das23c.pdf" style="color: #0000FF">here</a>. 
    
* "**Differentially Private Federated Learning with Normalized Updates**" - **R Das**, A Hashemi, S Sanghavi, and I S Dhillon.

    Download preprint <a href="https://arxiv.org/pdf/2106.07094.pdf" style="color: #0000FF">here</a>. Short version presented in OPT2022 workshop of NeurIPS 2022; download <a href="https://openreview.net/pdf?id=0FllaTqjor7" style="color: #0000FF">here</a>.

* "**Faster Non-Convex Federated Learning via Global and Local Momentum**" - **R Das**, A Acharya, A Hashemi, S Sanghavi, I S Dhillon, and U Topcu.

    UAI 2022. Download paper <a href="https://proceedings.mlr.press/v180/das22b.html" style="color: #0000FF">here</a> and preprint <a href="https://arxiv.org/pdf/2012.04061.pdf" style="color: #0000FF">here</a>.

* "**On the Benefits of Multiple Gossip Steps in Communication-Constrained Decentralized Optimization**" - A Hashemi, A Acharya\*, **R Das**\*, H Vikalo, S Sanghavi, and I S Dhillon (\* denotes equal contribution).

    IEEE Transactions on Parallel and Distributed Systems. Download paper <a href="https://ieeexplore.ieee.org/abstract/document/9664349" style="color: #0000FF">here</a> and preprint <a href="https://arxiv.org/pdf/2011.10643.pdf" style="color: #0000FF">here</a>.
    
* "**On the Convergence of a Biased Version of Stochastic Gradient Descent**" - **R Das**, J Zhang, and I S Dhillon.

     NeurIPS 2019 Beyond First Order Methods in ML workshop. Download paper <a href="https://drive.google.com/file/d/1fn1iO_CXWJzgVqiY3p84NfCf__i6hKCW/view" style="color: #0000FF">here</a>.

* "**On the Separability of Classes with the Cross-Entropy Loss Function**" - **R Das** and S Chaudhuri.

    Preprint. Download <a href="https://arxiv.org/abs/1909.06930" style="color: #0000FF">here</a>.
    
* "**Nonlinear Blind Compressed Sensing under Signal-Dependent Noise**" - **R Das** and A Rajwade.

    IEEE International Conference on Image Processing (ICIP) 2019. Download paper <a href="https://ieeexplore.ieee.org/abstract/document/8803173" style="color: #0000FF">here</a>.

* "**Sparse Kernel PCA for Outlier Detection**" - **R Das**, A Golatkar, and S Awate.

    IEEE International Conference on Machine Learning and Applications (ICMLA) 2018 Oral. Download paper <a href="https://arxiv.org/abs/1809.02497" style="color: #0000FF">here</a>.

* **iFood Challenge, FGVC Workshop, CVPR 2018** - P Kothari\*, A Sadhu\*, A Golatkar\*, and **R Das**\* (\* denotes equal contribution).

    Finished $2^{nd}$ in the public leaderboard and $3^{rd}$ in the private leaderboard (Team name: Invincibles). <a href="https://www.kaggle.com/c/ifood2018/leaderboard" style="color: #0000FF">Leaderboard Link</a>.
    Invited to present our method at CVPR 2018 (<a href="https://drive.google.com/file/d/1ycgDwlw62mWgaLy5qslvqjyiND0vgYTG/view?usp=sharing" style="color: #0000FF">slides</a>).

**<font size="+3">Internships</font>**

* **Student Researcher at Google Research, New York City, NY** (June '24 - August '24) <br/>
    *Host: Kyriakos Axiotis* <br/>
    * Worked on improving the quality of pruned large language models.  <br/>

* **Student Researcher at Google Research (Remote)** (November '23 - March '24) <br/>
    *Host: Alessandro Epasto* <br/>
    * Theoretically showed that retraining with predicted hard labels improves model accuracy in the presence of label noise. Empirically we showed that retraining appropriately can significantly improve training with label differential privacy.  <br/>
    
* **Student Researcher at Google DeepMind, Princeton, NJ** (June '23 - October '23) <br/>
    *Host: Naman Agarwal* <br/>
    * Derived new theoretical results to quantify the preconditioning effect of the Adam optimizer, and empirically benchmarked several optimization algorithms based on Adam. <br/>
    
* **Research Intern at Google (Remote)** (June '21 - August '21) <br/>
    *Hosts: Zheng Xu, Satyen Kale, and Tong Zhang* <br/>
    * Clipped gradient methods are commonly used in practice for differentially private (DP) training, e.g., DP-SGD. However, a sound theoretical understanding of these methods has been elusive. We provide principled guidance on choosing the clipping threshold in DP-SGD and also derive novel convergence results for DP-SGD in heavy-tailed settings. <br/>

* **Applied Scientist Intern at Amazon Search, Berkeley, CA** (May '20 - August '20) <br/>
    *Mentor: Dan Hill, Manager: Sujay Sanghavi* <br/>
    * Worked on customer-specific query correction by leveraging the "session data" (i.e. previous searches of the customer) using SOTA Transformer models. Our model generated better candidates than the production system. <br/>

* **Institute for Biomechanics, ETH Zürich, Zürich, Switzerland** (May '17 - July '17) <br/>
    *Guide : Dr. Patrik Christen and Prof. Dr. Ralph Müller, D-HEST* <br/>
    * Proposed a stable linear model (with closed-form solution) and a fuzzy boolean network for bone remodeling. Also developed an automated 2D-3D image registration framework for histology images from scratch. <br/>
