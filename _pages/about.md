---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Howdy!

I'm a third year PhD student in the Computer Science Department of **UT Austin**. I'm fortunate to be advised by Prof. **Inderjit Dhillon** and Prof. **Sujay Sanghavi**. Broadly, I'm interested in the design of efficient and provable machine learning and deep learning algorithms. Specifically, my current research interests include large-scale optimization, federated learning, and machine learning theory.

Previously, I was a dual degree (combined bachelor's and master's degree) student in the Department of Electrical Engineering, **Indian Institute of Technology** (**IIT**) **Bombay**. At IIT Bombay, I've worked under the guidance of Prof. **Subhasis Chaudhuri** on some probabilistically provable theoretical aspects of neural networks and algorithmic aspects of large-scale optimization for my final thesis <a href="https://drive.google.com/open?id=1gOwSCWhbJLVFN2K178Ujve-WxFv_VDdB">[link]</a>. I was awarded the **Undergraduate Research Award** (URA-03) for exceptional work in my final thesis.

You can check out my CV [here]({{ site.url }}/assets/CV_may.pdf).

**<font size="+3">Publications and Preprints</font>**

* "**Beyond Uniform Lipschitz Condition in Differentially Private Optimization**" - **Rudrajit Das**, Satyen Kale, Zheng Xu, Tong Zhang, Sujay Sanghavi.

    Submitted (Under Review).

* "**On the Convergence of Differentially Private Federated Learning on Non-Lipschitz Objectives, and with Normalized Client Updates**" - **Rudrajit Das**, Abolfazl Hashemi, Sujay Sanghavi, Inderjit S. Dhillon.

    Preprint (Under Review). Download <a href="https://arxiv.org/pdf/2106.07094.pdf" style="color: #0000FF">here</a>.

* "**Faster Non-Convex Federated Learning via Global and Local Momentum**" - **Rudrajit Das**, Anish Acharya, Abolfazl Hashemi, Sujay Sanghavi, Inderjit S. Dhillon, Ufuk Topcu.

    Accepted for poster presentation in The Conference on Uncertainty in Artificial Intelligence (UAI) 2022. Download preprint <a href="https://arxiv.org/pdf/2012.04061.pdf" style="color: #0000FF">here</a>.

* "**On the Benefits of Multiple Gossip Steps in Communication-Constrained Decentralized Optimization**" - Abolfazl Hashemi, Anish Acharya^, **Rudrajit Das**^, Haris Vikalo, Sujay Sanghavi, Inderjit Dhillon (^ denotes equal contribution).

    Accepted in IEEE Transactions on Parallel and Distributed Systems. Download paper <a href="https://ieeexplore.ieee.org/abstract/document/9664349" style="color: #0000FF">here</a> and preprint <a href="https://arxiv.org/pdf/2011.10643.pdf" style="color: #0000FF">here</a>.
    
* "**On the Convergence of a Biased Version of Stochastic Gradient Descent**" - **Rudrajit Das**, Jiong Zhang and Inderjit Dhillon.

    Accepted for poster presentation in "Beyond First Order Methods in ML" workshop in NeurIPS 2019. Download paper <a href="https://drive.google.com/file/d/1fn1iO_CXWJzgVqiY3p84NfCf__i6hKCW/view" style="color: #0000FF">here</a>.

* "**On the Separability of Classes with the Cross-Entropy Loss Function**" - **Rudrajit Das** and Subhasis Chaudhuri.

    Preprint. Download <a href="https://arxiv.org/abs/1909.06930" style="color: #0000FF">here</a>.
    
* "**Nonlinear Blind Compressed Sensing under Signal-Dependent Noise**" - **Rudrajit Das** and Ajit Rajwade.

    Accepted for presentation in IEEE International Conference on Image Processing (ICIP) 2019. Download paper <a href="https://ieeexplore.ieee.org/abstract/document/8803173" style="color: #0000FF">here</a>.

* "**Sparse Kernel PCA for Outlier Detection**" - **Rudrajit Das**, Aditya Golatkar and Suyash Awate.

    Accepted for oral presentation in IEEE International Conference on Machine Learning and Applications (ICMLA) 2018. Download paper <a href="https://arxiv.org/abs/1809.02497" style="color: #0000FF">here</a>.

* **iFood Challenge, FGVC Workshop, CVPR 2018** - Parth Kothari^, Arka Sadhu^, Aditya Golatkar^, **Rudrajit Das**^ (^ denotes equal contribution).

    Finished $2^{nd}$ in the public leaderboard and $3^{rd}$ in the private leaderboard (Team name : Invincibles). <a href="https://www.kaggle.com/c/ifood2018/leaderboard" style="color: #0000FF">Leaderboard Link</a>.
    Invited to present our method at CVPR 2018 (slides can be found  <a href="https://drive.google.com/file/d/1ycgDwlw62mWgaLy5qslvqjyiND0vgYTG/view?usp=sharing" style="color: #0000FF">here</a>).

**<font size="+3">Internships</font>**

* **Research Intern at Google (Virtual)** (June '21 - Aug '21) <br/>
    *Mentors: Zheng Xu, Satyen Kale and Tong Zhang* <br/>
    * Clipped gradient methods are commonly used in practice for improving convergence as well as for differentially private (DP) training. However, a sound theoretical understanding of these methods has been elusive. We are working on deriving better theory for (DP-)SGD with clipping via the lens of bias-variance tradeoff of a gradient estimator. <br/>

* **Applied Scientist Intern at Amazon Search (Virtual), Berkeley, USA** (May '20 - Aug '20) <br/>
    *Mentor: Dan Hill, Manager: Sujay Sanghavi* <br/>
    * Worked on customer-specific query correction by leveraging the "session data" (i.e. previous searches of the customer) using SOTA Transformer models. Our model generated better candidates than the production system. <br/>

* **Institute for Biomechanics, ETH Zürich, Switzerland** (May '17 - July '17) <br/>
    *Guide : Dr. Patrik Christen and Prof. Dr. Ralph Müller, D-HEST* <br/>
    * Proposed a stable linear model (with closed form solution) and a fuzzy boolean network for bone re-modeling. Also developed an automated 2D-3D image registration framework for histology images from scratch. <br/>
